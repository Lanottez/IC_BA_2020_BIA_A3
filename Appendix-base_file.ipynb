{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to build you code based on this one, so it will be easier for us to intergate all the codes into one file\n",
    "# Create a function for each plot that you want to call\n",
    "# Replace the screenshot in final report to the function you want to call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# This cell should applies to the code of all of us\n",
    "data_path = \"./data/\"\n",
    "subscriptions = pd.read_csv(data_path + 'subscriptionsdataFINAL.csv') \n",
    "visitation_part1 = pd.read_csv(data_path + 'visitationdataPART1.csv') \n",
    "visitation_part2 = pd.read_csv(data_path + 'visitationdataPART2.csv') #convert the join and end date to_date\n",
    "subscriptions['Join Date'] =  pd.to_datetime(subscriptions['Join Date'].str.upper(), format='%b-%y', yearfirst=False)\n",
    "subscriptions['Join_year'] = pd.DatetimeIndex(subscriptions['Join Date']).year\n",
    "subscriptions['End Date'] =  pd.to_datetime(subscriptions['End Date'].str.upper(), format='%b-%y', yearfirst=False)\n",
    "subscriptions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the seperate dataset to avoid any variable name conflict \n",
    "subscriptions_Qian =  subscriptions.copy()\n",
    "visitation_part1_Qian = visitation_part1.copy() \n",
    "visitation_part2_Qian = visitation_part2.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing - Qian\n",
    "df = subscriptions_Qian.copy()\n",
    "numbers_club = pd.DataFrame(df.groupby('Join_year')['Club'].nunique())\n",
    "numbers_club['Growth']=numbers_club['Club'].pct_change()\n",
    "member_joined_data = subscriptions.groupby(['Join Date','Subscription Type']).size().reset_index(name='Number_of_member_joined')\n",
    "member_joined_data = member_joined_data.rename(columns={'Join Date': \"Date\"})\n",
    "member_left_data = subscriptions.groupby(['End Date','Subscription Type']).size().reset_index(name='Number_of_member_left')\n",
    "member_left_data=member_left_data.rename(columns={'End Date': \"Date\"})\n",
    "member_left_data['Date']=member_left_data['Date'] + pd.DateOffset(months=1)\n",
    "member_data = pd.merge(member_joined_data,member_left_data,on = ['Subscription Type','Date'],how='outer').sort_index().replace({np.nan:0})\n",
    "member_data['net_member'] = member_data['Number_of_member_joined'] - member_data['Number_of_member_left'] \n",
    "member_data = member_data.drop(columns = ['Number_of_member_joined', 'Number_of_member_left'])\n",
    "Cumulative_Members = pd.DataFrame(member_data.groupby(['Date']).sum('net_member').unstack(fill_value=0))\n",
    "Cumulative_Members = Cumulative_Members.rename(columns={0: \"Cumulative_members\"})\n",
    "Cumulative_Members = Cumulative_Members.cumsum().reset_index()\n",
    "Cumulative_Members_yearly = Cumulative_Members.copy()\n",
    "Cumulative_Members_yearly['year'] = pd.DatetimeIndex(Cumulative_Members_yearly['Date']).year\n",
    "Cumulative_Members_yearly=Cumulative_Members_yearly.groupby(['year']).sum()\n",
    "Cumulative_Members_yearly=Cumulative_Members_yearly.drop([2019])\n",
    "Cumulative_Members_yearly['Cumulative_members_Grwoth']=Cumulative_Members_yearly['Cumulative_members'].pct_change()\n",
    "Cumulative_Members_seperating = pd.DataFrame(member_data.groupby(['Date','Subscription Type']).sum('net_member').unstack(fill_value=0))\n",
    "Cumulative_Members_seperating = Cumulative_Members_seperating.rename(columns={0: \"Cumulative_members\"})\n",
    "Cumulative_Members_seperating = Cumulative_Members_seperating.cumsum().reset_index()\n",
    "x = Cumulative_Members_seperating['net_member']\n",
    "y = pd.DataFrame({'Date':Cumulative_Members_seperating['Date']})\n",
    "y['Flexible']=x['Flexible']\n",
    "y['Standard']=x['Standard']\n",
    "Cumulative_Members_seperating=y\n",
    "def calculate_revenue_flexible(Date,net_member):\n",
    "    date_while_price_changed = pd.Timestamp('2016-01-01') \n",
    "    if Date < date_while_price_changed:\n",
    "        return net_member * 25.99\n",
    "    elif Date >= date_while_price_changed:\n",
    "        return net_member * 30.99\n",
    "def calculate_revenue_standard(Date,net_member):\n",
    "    date_while_price_changed = pd.Timestamp('2016-01-01') \n",
    "    if Date < date_while_price_changed:\n",
    "        return net_member * 20.99\n",
    "    elif Date >= date_while_price_changed:\n",
    "        return net_member * 26.99 \n",
    "Cumulative_Members_seperating_revenue = Cumulative_Members_seperating.copy()\n",
    "Cumulative_Members_seperating_revenue['Flexible_revenue']=Cumulative_Members_seperating_revenue.apply(lambda row: calculate_revenue_flexible(row['Date'], row['Flexible']),axis=1)\n",
    "Cumulative_Members_seperating_revenue['Standard_revenue']=Cumulative_Members_seperating_revenue.apply(lambda row: calculate_revenue_standard(row['Date'], row['Standard']),axis=1)\n",
    "Cumulative_Members_seperating_revenue['Total_revenue']=Cumulative_Members_seperating_revenue['Flexible_revenue'] + Cumulative_Members_seperating_revenue['Standard_revenue']\n",
    "Cumulative_Members_seperating_revenue_yearly = Cumulative_Members_seperating_revenue.copy()\n",
    "Cumulative_Members_seperating_revenue_yearly['year'] = pd.DatetimeIndex(Cumulative_Members_seperating_revenue_yearly['Date']).year\n",
    "Cumulative_Members_seperating_revenue_yearly=Cumulative_Members_seperating_revenue_yearly.groupby(['year']).sum()\n",
    "Cumulative_Members_seperating_revenue_yearly=Cumulative_Members_seperating_revenue_yearly.drop([2019])\n",
    "Cumulative_Members_seperating_revenue_yearly['Revenue_grwoth']=Cumulative_Members_seperating_revenue_yearly['Total_revenue'].pct_change()\n",
    "subscriptions_2 = subscriptions_Qian.copy()\n",
    "date_min = min (subscriptions_2['Join Date'].min(),subscriptions_2['End Date'].min() )\n",
    "date_max = max (subscriptions_2['Join Date'].max(),subscriptions_2['End Date'].max() )\n",
    "dates = pd.date_range(date_min, date_max, freq = 'MS')\n",
    "subscriptions_2['Duration'] = np.where(subscriptions_2['End Date'].isna(), \n",
    "                                     (date_max - subscriptions_2['Join Date']), \n",
    "                                     (subscriptions_2['End Date'] - subscriptions_2['Join Date']) )\n",
    "subscriptions_2['Duration'] = (subscriptions_2['Duration'] /  np.timedelta64(1, 'M') ).apply(math.floor)\n",
    "subscriptions_3= subscriptions_2.copy()\n",
    "visitation_part1_peakvisit = visitation_part1.groupby(['CustomerID', 'peakvisits']).size().unstack(fill_value=0)\n",
    "visitation_part2_peakvisit = visitation_part2.groupby(['CustomerID', 'peakvisits']).size().unstack(fill_value=0)\n",
    "visitation_peakvisit = pd.merge(visitation_part1_peakvisit,visitation_part2_peakvisit,on='CustomerID',how='outer').sort_index().replace({np.nan:0})\n",
    "visitation_peakvisit['False'] = visitation_peakvisit['False_x'] + visitation_peakvisit['False_y']\n",
    "visitation_peakvisit['True'] = visitation_peakvisit['True_x'] + visitation_peakvisit['True_y']\n",
    "visitation_peakvisit=visitation_peakvisit.drop(columns=['False_x','True_x','False_y','True_y'])\n",
    "visitation_peakvisit['Peak_proportion'] = visitation_peakvisit['True']/(visitation_peakvisit['True'] + visitation_peakvisit['False'])\n",
    "def classify_peak(Peak_proportion):\n",
    "    if Peak_proportion < 0.35:\n",
    "        return 'Off_Peak_Customer'\n",
    "    elif 0.25 <= Peak_proportion < 0.65:\n",
    "        return 'Average'\n",
    "    else:\n",
    "        return 'Peak_Customer'\n",
    "visitation_peakvisit['Peak_categogy'] = visitation_peakvisit['Peak_proportion'].apply(lambda x : classify_peak(x))\n",
    "\n",
    "def plot_KNN_model(size_dataset,k_value):\n",
    "    subscriptions_3= subscriptions_2.copy()\n",
    "    subscriptions_3 = pd.merge(subscriptions_3,visitation_peakvisit['Peak_categogy'],on='CustomerID',how='outer')\n",
    "    subscriptions_3 = subscriptions_3[:size_dataset]\n",
    "    subscriptions_4=subscriptions_3.copy().drop(columns=['End Date','CustomerID'])\n",
    "    def normalises_string(data,data_list):\n",
    "        index = data_list.index(data)\n",
    "        return index\n",
    "    for column in subscriptions_4.columns:\n",
    "        if column != 'Duration':\n",
    "            subscriptions_4[column] = subscriptions_4.apply(lambda row: normalises_string(row[column], list(subscriptions_4[column].unique())),axis=1)\n",
    "    if size_dataset == 800:\n",
    "        training_data, validation_data, test_data = subscriptions_4[:400], subscriptions_4[400:600], subscriptions_4[600:] \n",
    "    else: #data_set = 4000\n",
    "        training_data, validation_data, test_data = subscriptions_4[:2000], subscriptions_4[2000:3000], subscriptions_4[3000:] \n",
    "    def normalises_the_data_z_score(input_data):\n",
    "        data_normalised = preprocessing.scale(input_data)\n",
    "        data_normalised = pd.DataFrame(data=data_normalised,columns = list(input_data.columns))\n",
    "        data_normalised['Duration'] = input_data['Duration']\n",
    "        return data_normalised\n",
    "    training_data_normalised = normalises_the_data_z_score(training_data)\n",
    "    validation_data_normalised = normalises_the_data_z_score(validation_data)\n",
    "    test_data_normalised = normalises_the_data_z_score(test_data)\n",
    "    training_data_normalised_x = training_data_normalised.copy().drop(columns=['Duration']).values\n",
    "    training_data_normalised_y = training_data['Duration']\n",
    "\n",
    "    validation_data_normalised_x = validation_data_normalised.copy().drop(columns=['Duration']).values\n",
    "    validation_data_normalised_y = validation_data['Duration']\n",
    "    scores_list = []\n",
    "    for k in range(1,k_value):\n",
    "       classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "       classifier.fit(training_data_normalised_x, training_data_normalised_y)\n",
    "       validation_data_normalised_y_predicted = classifier.predict(validation_data_normalised_x)\n",
    "       scores_list.append(1-metrics.accuracy_score(validation_data_normalised_y,validation_data_normalised_y_predicted,normalize=True))\n",
    "    scores_list_df = pd.DataFrame(data=scores_list).rename(columns={0: \"Classificaion error(validation data)\"})\n",
    "    scores_list_df['k-value'] = list(range(1,k_value))\n",
    "    scores_list = []\n",
    "    # The part below is to use the trainning data to predict the training data.\n",
    "    for k in range(1,k_value):\n",
    "       classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "       classifier.fit(training_data_normalised_x, training_data_normalised_y)\n",
    "       training_data_normalised_y_predicted = classifier.predict(training_data_normalised_x)\n",
    "       scores_list.append(1-metrics.accuracy_score(training_data_normalised_y,training_data_normalised_y_predicted,normalize=True))\n",
    "    scores_list_df['Classificaion error(training data)'] = scores_list\n",
    "    plt.plot(scores_list_df['k-value'], scores_list_df['Classificaion error(validation data)'], label = \"Validation data\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    if size_dataset==800:\n",
    "        plt.title('Classification error, 800 rows, k=1-100')\n",
    "    else:\n",
    "        plt.title('Classification error, 4000 rows, k=1-200')\n",
    "    plt.ylabel('%')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot graph - Qian\n",
    "def plot_growth_of_new_members():\n",
    "    Year = Cumulative_Members_yearly.index.values\n",
    "    Cumulative_members_Grwoth = Cumulative_Members_yearly['Cumulative_members_Grwoth']\n",
    "    plt.plot(Year,Cumulative_members_Grwoth)\n",
    "    plt.ylabel('%')\n",
    "    plt.title('Growth of New Members')\n",
    "def plot_cumulative_active_members():\n",
    "    Date = Cumulative_Members.Date\n",
    "    Cumulative_members = Cumulative_Members.Cumulative_members\n",
    "    plt.plot(Date[:-1],Cumulative_members[:-1])\n",
    "    plt.title('Cumulative Active Members')\n",
    "    plt.axvline(x='2016-01-01')\n",
    "def plot_monthly_revenue():\n",
    "    Date = Cumulative_Members_seperating_revenue['Date']\n",
    "    Total = Cumulative_Members_seperating_revenue['Total_revenue']\n",
    "    plt.plot(Date[:-1],Total[:-1],label = 'Total')\n",
    "    plt.axvline(x='2016-01-01')\n",
    "    plt.ylabel('Pound')\n",
    "    plt.title('Monthly Revenue')\n",
    "def plot_growth_yearly_revenue():\n",
    "    Date = Cumulative_Members_seperating_revenue_yearly.index.values\n",
    "    Revenue_grwoth = Cumulative_Members_seperating_revenue_yearly['Revenue_grwoth']\n",
    "    plt.plot(Date,Revenue_grwoth)\n",
    "    plt.ylabel('%')\n",
    "    plt.title('Yearly Revenue Growth')\n",
    "def plot_bar_yearly_revenue():\n",
    "    r =  np.arange(2009,2019)\n",
    "    Total_revenue = Cumulative_Members_seperating_revenue_yearly['Total_revenue']\n",
    "    plt.ylabel('Pound')\n",
    "    plt.bar(r,Total_revenue)\n",
    "    plt.title('Yearly Revenue')\n",
    "def plot_number_of_club(): \n",
    "    plt.bar(numbers_club.index.values, numbers_club['Club'])\n",
    "    plt.ylabel('Number of Clubs')\n",
    "    plt.title('Number of clubs')\n",
    "def plot_KNN_model_800():\n",
    "    plot_KNN_model(800,101)\n",
    "def plot_KNN_model_4000():\n",
    "    plot_KNN_model(4000,201)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cumulative_Members_seperating_revenue_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
